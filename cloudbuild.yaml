# Clone the 

steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      echo "Retrieving secret..."
      gcloud secrets versions access latest --secret=${_GITHUB_DEPLOY_KEY} > ${_SECRET_KEY} || exit 1
      # set permissions to 600
      chmod 600 ${_SECRET_KEY}

  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      echo "Setting name and email on git configs"
      git config --global user.email "iteng@confluent.io"
      git config --global user.name "IT Engineering"
      echo "Add GitHub HostKey..."
      ssh-keyscan -t rsa github.com > known_hosts.github
      echo "Adding ssh key..."
      eval "$(ssh-agent)"
      ssh-add ${_SECRET_KEY}
      ssh-add -l
      echo "Cloning repository..."
      GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" git clone --single-branch --branch=master git@github.com:confluentinc/it-eng.git
      echo "Fetch git tags..."
      cd ${_BASE_REPO_PATH}
      GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" git fetch --tags

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      set -e
      set -o pipefail

      # Install jq
      apt-get update && apt-get install -y jq

      cd ${_BASE_REPO_PATH}/${_REPO_PATH}/
      # File that contains failures.
      failure_file=failure.log
      touch ${failure_file}

      # Loop through the builders, and build independently.
      for d in */; do
        echo "Path: ^${_REPO_PATH}/${d}"

        # Check if there are any files have been changed
        check_changes=$(python3 ${_BASE_REPO_PATH}/scripts/binary/merge_changes.py "^${_REPO_PATH}/${d}")

        if [[ "$check_changes" == "match" ]]; then
          # Check if the deployable file exist
          config="${_BASE_REPO_PATH}/${_REPO_PATH}/${d}deploy.json"
          if [[ ! -f "${config}" ]]; then
            echo "${_BASE_REPO_PATH}/${_REPO_PATH}/${d}: ${config} file not found." | tee -a ${failure_file}
            continue
          fi

          echo "#############################################"
          echo "Parsing deploy.json ... "
          echo "#############################################"
          cat ${_BASE_REPO_PATH}/${_REPO_PATH}/${d}deploy.json | jq -r '.job | "_JOB_NAME=\(.name)\n_SERVICE_ACCOUNT=\(.service_account)\n_REGION=\(.region)\n_MAIN_FUNCTION=\(.deployment.main_function)\n_REPO=\(.deployment.repo)"' > /workspace/deploy.env

          set -a && source /workspace/deploy.env && set +a
          cat /workspace/deploy.env
   
          job_name=$(grep -E "^_JOB_NAME=" /workspace/deploy.env | cut -d"=" -f2-)
          service_account=$(grep -E "^_SERVICE_ACCOUNT=" /workspace/deploy.env | cut -d"=" -f2-)
          region=$(grep -E "^_REGION=" /workspace/deploy.env | cut -d"=" -f2-)
          main_function=$(grep -E "^_MAIN_FUNCTION=" /workspace/deploy.env | cut -d"=" -f2-)
          repo=$(grep -E "^_REPO=" /workspace/deploy.env | cut -d"=" -f2-)

          if [[ -s ${_BASE_REPO_PATH}/${_REPO_PATH}/${d}deploy.json ]]; then
            echo "#############################################"
            echo "Building and deploying $d ... "
            echo "#############################################"
            # Logfile
            logfile="${d::-1}.log" # Logfile for "foo/" builder is "foo.log".

            # Deploy the function
            #gcloud functions call ${_FUNCTION_NAME} --data "$(cat ${config})" > ${logfile} 2>&1
            echo "gcloud functions deploy ${job_name} \
                --trigger-http \
                --entry-point=${main_function} \
                --memory=${_MEMORY} \
                --region=${region} \
                --runtime=${_RUNTIME} \
                --timeout=${_TIMEOUT} \
                --service-account="${service_account}@${PROJECT_ID}.iam.gserviceaccount.com" \
                --env-vars-file="${_BASE_REPO_PATH}/${_REPO_PATH}/${d}${_ENV_VARS_FILE}" \
                --source="${_BASE_REPO_PATH}/${_REPO_PATH}/${d}" \
                --vpc-connector=${_VPC_CONNECTOR} \
                --ingress-settings=${_INGRESS_SETTINGS} \
                --egress-settings=${_EGRESS_SETTINGS} "

            gcloud functions deploy ${job_name} \
                --trigger-http \
                --entry-point=${main_function} \
                --memory=${_MEMORY} \
                --region=${region} \
                --runtime=${_RUNTIME} \
                --timeout=${_TIMEOUT} \
                --service-account="${service_account}@${PROJECT_ID}.iam.gserviceaccount.com" \
                --env-vars-file="${_BASE_REPO_PATH}/${_REPO_PATH}/${d}${_ENV_VARS_FILE}" \
                --source="${_BASE_REPO_PATH}/${_REPO_PATH}/${d}" \
                --vpc-connector=${_VPC_CONNECTOR} \
                --ingress-settings=${_INGRESS_SETTINGS} \
                --egress-settings=${_EGRESS_SETTINGS} 
            #> ${logfile} 2>&1

            # Mark there a build
            echo "$d" > /workspace/deployed_function.log

            # Append to fail to log file if exit code != 0
            if [[ $? -ne 0 ]]; then
              echo "$d failed" | tee -a ${failure_file}
              cat ${logfile}
            fi
          else
            echo "Did not find deploy.json file in ${d}"
            continue
          fi
        else
          echo "No changes in ${d}"
        fi
      done

      wait
      # Check if there is any failure.
      if [[ -s ${failure_file} ]]; then
        echo
        echo "following builds have failed: "
        cat ${failure_file}
        echo "Exiting."
        exit 1
      fi

      # Cleanup
      rm -f ${keyfile}

      echo "All builds succeeded."

  - name: 'python'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      cd ${_BASE_REPO_PATH}
      if [ -s /workspace/deployed_function.log ]; then
        echo "Increment git tag..."
        python3 ${_BASE_REPO_PATH}/scripts/binary/increment_tag.py -e=${PROJECT_ID} > ${_NEW_TAG} || exit 1
        cat ${_NEW_TAG}
      else
        echo "No function has been deployed."
      fi


  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      if [ -s ${_NEW_TAG} ]; then
        #You must head over to this repo
        cd ${_BASE_REPO_PATH}

        echo "File, ${_NEW_TAG} exists."

        echo "Setting name and email on git configs"
        git config --global user.email "iteng@confluent.io"
        git config --global user.name "IT Engineering"

        echo "Add GitHub HostKey..."
        ssh-keyscan -t rsa github.com > known_hosts.github

        echo "Adding ssh key..."
        eval "$(ssh-agent)"
        ssh-add ${_SECRET_KEY}
        ssh-add -l

        echo "get current tag version"
        new_tag=$(cat ${_NEW_TAG})

        echo "Creating and pushing new tag $new_tag"
        GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" git tag -a $new_tag -m "$new_tag" || exit 1
        GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" git push origin $new_tag || exit 1

        echo "Remove secret key"
        rm -f ${_SECRET_KEY}
      else
        echo "The tag, ${_NEW_TAG} does not exist."
      fi

substitutions:
  _BASE_REPO_PATH: '/workspace/it-eng'
  _REPO_PATH: 'scripts/gcloud-v1.1'
  _SECRET_KEY: '/workspace/secret_key'
  _GITHUB_DEPLOY_KEY: gcp-build-github-deploy-key
  _NEW_TAG: /workspace/new_tag.txt
  _ENV_VARS_FILE: helper
  _MEMORY: '512'
  _TIMEOUT: '540'
  _VPC_CONNECTOR: gcp-function-connector
  _INGRESS_SETTINGS: internal-only
  _EGRESS_SETTINGS: all
  _RUNTIME: python311


options:
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET
